{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggleの以下のURLを参考に、metric learningを実装\n",
    "# https://www.kaggle.com/bestaar/deep-metric-learning-with-pretrained-keras-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  triplet networks can explicitely learn similarity between inputs:\n",
    "#  -> Hoffer, Elad, and Nir Ailon. \"Deep metric learning using triplet network.\" International Workshop on Similarity-Based Pattern Recognition. Springer, Cham, 2015.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "# Here is a script for loading pretrained models in keras to finetune them in a triplet network setting\n",
    "from keras.layers import Input,Lambda,subtract,GlobalMaxPooling2D,Dense,GlobalAveragePooling2D,concatenate,Activation,Flatten\n",
    "from keras.applications.xception import Xception as Net\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.models import Model,load_model\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Util import NormalizeHorizontalDirection\n",
    "\n",
    "from project_utility import f1,f1_loss,SpectrogramGenerator, change_pitch,change_speed,change_speed_and_pitch,add_noise,slightly_timeshift,stfft,zero_padding,calc_spectrogram\n",
    "\n",
    "from resnet import compose,ResNetConv2D,ResnetBuilder,shortcut,basic_block,residual_blocks,bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作業ディレクトリの設定\n",
    "os.chdir(\"/home/taichi/DataAnalysis/05_NTT_corevo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数のseedを固定\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    ## Input shape\n",
    "    d1 = input_shape[0]\n",
    "    d2 = input_shape[1]\n",
    "        \n",
    "    # The triplet network takes 3 input images: 2 of the same class and 1 out-of-class sample\n",
    "    input_tensor1 = Input(shape=(d1, d2, 1))\n",
    "    input_tensor2 = Input(shape=(d1, d2, 1))\n",
    "    input_tensor3 = Input(shape=(d1, d2, 1))\n",
    "    \n",
    "    # load a pretrained model (try, except block because the kernel would not let me download the weights for the network)\n",
    "    try:\n",
    "#        model_path = \"03_work/models/resnet_voxceleb/resnet_voxceleb_model_time_500.h5\"\n",
    "        model_path = \"03_work/models/fine_tuning/fine_tuning_model.h5\"\n",
    "       \n",
    "        trained_model = load_model(model_path,\n",
    "                                           custom_objects={\"f1\":f1})\n",
    "    \n",
    "#        trained_model = ResnetBuilder.build_resnet_50([d1,d2,1], 6)\n",
    "        \n",
    "        layer_name = 'activation_49'\n",
    "        base_model = Model(inputs=trained_model.input, \n",
    "                                   outputs=trained_model.get_layer(layer_name).output)\n",
    "        \n",
    "#        base_model = Net(input_shape=(d1,d2,3),weights='imagenet',include_top=False)\n",
    "\n",
    "    except:\n",
    "        print('Could not download weights. Using random initialization...')\n",
    "       # base_model = Net(input_shape=(d1,d2,3),weights=None,include_top=False)\n",
    "    \n",
    "    # predefine a summation layer for calculating the distances:\n",
    "    # the weights of this layer will be set to ones and fixed  (since they\n",
    "    # are shared we could also leave them trainable to get a weighted sum)\n",
    "    summation = Dense(1,activation='linear',kernel_initializer='ones',bias_initializer='zeros',name='summation')\n",
    "    # feed all 3 inputs into the pretrained keras model\n",
    "    x1 = base_model(input_tensor1)\n",
    "    x2 = base_model(input_tensor2)\n",
    "    x3 = base_model(input_tensor3)\n",
    "    # flatten/summarize the models output:\n",
    "    # (here we could also use GlobalAveragePooling or simply Flatten everything)\n",
    "    \n",
    "    x1 = GlobalMaxPooling2D()(x1)\n",
    "    x2 = GlobalMaxPooling2D()(x2)\n",
    "    x3 = GlobalMaxPooling2D()(x3)\n",
    "#    x1 = Flatten()(x1)\n",
    " #   x2 = Flatten()(x2)\n",
    "  #  x3 = Flatten()(x3)\n",
    "    \n",
    "    # calculate something proportional to the euclidean distance\n",
    "    #   a-b\n",
    "    d1 = subtract([x1,x2])\n",
    "    d2 = subtract([x1,x3])\n",
    "    #   (a-b)**2\n",
    "    d1 = Lambda(lambda val: val**2)(d1)\n",
    "    d2 = Lambda(lambda val: val**2)(d2)\n",
    "    # sum((a-b)**2)\n",
    "    d1 = summation(d1)\n",
    "    d2 = summation(d2)\n",
    "    #  concatenate both distances and apply softmax so we get values from 0-1\n",
    "    d = concatenate([d1,d2])\n",
    "    d = Activation('softmax')(d)\n",
    "    # build the model and show a summary\n",
    "    model = Model(inputs=[input_tensor1,input_tensor2,input_tensor3], outputs=d)\n",
    "    # a second model that can be used as metric between input 1 and input 2\n",
    "    metric = Model(inputs=[input_tensor1,input_tensor2], outputs=d1)\n",
    "\n",
    "    # draw the network (it looks quite nice)\n",
    "    try:\n",
    "        from keras.utils.vis_utils import plot_model as plot\n",
    "        plot(model, to_file = 'Triplet_Dense121.png')\n",
    "    except ImportError:\n",
    "        print('It seems like the dependencies for drawing the model (pydot, graphviz) are not installed')\n",
    "    # fix the weights of the summation layer (since the weight of this layer\n",
    "    # are shared we could also leave them trainable to get a weighted sum)\n",
    "    for l in model.layers:\n",
    "        if l.name == 'summation':\n",
    "            print('fixing weights of summation layer')\n",
    "            l.trainable=False\n",
    "    # compile model\n",
    "    LEARNING_RATE = 0.02\n",
    "    DECAY = 1e-6\n",
    "    \n",
    "    sgd = SGD(lr=LEARNING_RATE, decay=DECAY, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "    \n",
    "    \n",
    "    ## base_modelは同じデータを参照している。 (ポインタ的な)\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer=sgd, \n",
    "                  loss='categorical_crossentropy')\n",
    "        \n",
    "    return model, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing weights of summation layer\n"
     ]
    }
   ],
   "source": [
    "input_shape = (512,500,1)\n",
    "model, metric = create_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric learning用のGeneratorを作成。\n",
    "class SpectrogramGeneratorMetricLearning():\n",
    "    \n",
    "    def __init__(self,lb,ub,train_info,speaker_dict,batch_size,time_len,test_info,milestone):\n",
    "        self.reset()\n",
    "\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        #self.train_info = train_info\n",
    "        # 学習する際は__init__の時点でcalc_spectrogram_funcを設定する必要はない。\n",
    "        self.calc_spectrogram_func = lambda filepath: calc_spectrogram(filepath,\n",
    "                                                               lb_ms=lb,ub_ms=ub,\n",
    "                                                               speed=False,pitch=False,\n",
    "                                                               time_shift=False,noise=False)\n",
    "        \n",
    "        X_train,X_val,y_train,y_val = train_test_split(np.array(train_info[\"filepath\"]),\n",
    "                                                       np.array(train_info[\"label\"]),\n",
    "                                                       test_size=0.1,\n",
    "                                                       random_state = 1234)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val        \n",
    "                \n",
    "        self.speaker_dict = speaker_dict\n",
    "        self.num_classes = len(speaker_dict)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.time_len = time_len        \n",
    "        # step_per_epoch\n",
    "        \n",
    "        self.train_step_per_epoch = int(np.ceil(len(X_train) / batch_size))\n",
    "        self.val_step_per_epoch = int(np.ceil(len(X_val) / batch_size))\n",
    "        \n",
    "        # テストデータ\n",
    "        X_test = np.array(test_info[\"filepath\"])\n",
    "        self.X_test = X_test\n",
    "        # y_testはダミーを加えておく\n",
    "        self.y_test = np.repeat(\"FE_AD\",len(test_info[\"filepath\"]))\n",
    "        self.test_step_per_epoch = int(np.ceil(len(X_test) / batch_size))        \n",
    "        \n",
    "        ## カリキュラム学習\n",
    "        self.milestones = milestones\n",
    "        \n",
    "        ## iterationの回数\n",
    "        self.train_cur_index = 0\n",
    "        self.val_cur_index = 0\n",
    "                \n",
    "    def reset(self):\n",
    "        self.batch1 = []\n",
    "        self.batch2 = []\n",
    "        self.batch3 = []\n",
    "        self.output= []\n",
    "    \n",
    "    def get_batch(self,X,y):\n",
    "        \n",
    "        # metric learningでは、3つのデータの距離d1,d2を出力にする\n",
    "        target_labels = np.random.choice(list(self.speaker_dict.keys()), \n",
    "                                 size = 2, replace = False)\n",
    "        \n",
    "        # 3つのデータのうち2つは共通のラベルで、1つは異なるラベルにする\n",
    "        match_label = target_labels[0]\n",
    "        diff_label = target_labels[1]\n",
    "#        print(match_label)\n",
    "        \n",
    "        \n",
    "        # データをpandas dataframeからランダムに抽出\n",
    "        match_data = np.random.choice(X[y == match_label],\n",
    "                                      size = 2,\n",
    "                                      replace = False)\n",
    "\n",
    "        diff_data = np.random.choice(X[y == diff_label],\n",
    "                                     size = 1,\n",
    "                                     replace = False)\n",
    "        \n",
    "        # データのpathをマージして、スペクトログラムを計算\n",
    "        merge_data = np.array([self.calc_spectrogram_func(path) for path in np.array([match_data[0],match_data[1],diff_data[0]])])\n",
    "        \n",
    "        # データのラベルをマージ\n",
    "#        merge_label = np.array([match_label,match_label,diff_label])\n",
    "        \n",
    "        distance = [0,1]\n",
    "                        \n",
    "        # データの順番をシャッフル ->　base_modelは共通なので、shuffleは必要ない。\n",
    "#        indexes = np.random.permutation(3)\n",
    "        \n",
    " #       merge_data = merge_data[indexes]\n",
    "  #      merge_label = merge_label[indexes]\n",
    "        \n",
    "        # d1, d2を計算\n",
    "#        if merge_label[0] == merge_label[1]:\n",
    "#           d1 = 0\n",
    "#      else:\n",
    " #           d1 = 1\n",
    "\n",
    "#        if merge_label[1] == merge_label[2]:\n",
    " #           d2 = 0\n",
    "  #      else:\n",
    "    #        d2 = 1\n",
    "    \n",
    "            \n",
    "        # distanceを計算 0-1に値が抑えるようする\n",
    "        \n",
    "        #f d1 == d2:\n",
    "            # d1:1, d2:1の場合しかない。\n",
    "         #   distance = [0.5,0.5]\n",
    "        #elif d1 > d2:\n",
    "         #   distance = [1,0]\n",
    "        #else:\n",
    "         #   distance = [0,1]\n",
    "                \n",
    "        return merge_data, distance\n",
    "\n",
    "    def next_train(self):\n",
    "        while True:\n",
    "            merge_input, distance = self.get_batch(self.X_train,self.y_train)\n",
    "            \n",
    "            self.batch1.append(merge_input[0])\n",
    "            self.batch2.append(merge_input[1])                                                                        \n",
    "            self.batch3.append(merge_input[2])\n",
    "                                                                      \n",
    "            self.output.append(distance)\n",
    "            \n",
    "            if len(self.output) == self.batch_size:\n",
    "                input1 = np.asarray(self.batch1, dtype=np.float32).reshape(self.batch_size,\n",
    "                                                                                   merge_input.shape[1],\n",
    "                                                                                    merge_input.shape[2],1)\n",
    "                \n",
    "                input2 = np.asarray(self.batch1, dtype=np.float32).reshape(self.batch_size,\n",
    "                                                                                   merge_input.shape[1],\n",
    "                                                                                    merge_input.shape[2],1)\n",
    "                \n",
    "                input3 = np.asarray(self.batch1, dtype=np.float32).reshape(self.batch_size,\n",
    "                                                                                   merge_input.shape[1],\n",
    "                                                                                    merge_input.shape[2],1)\n",
    "                \n",
    "                inputs = [input1,input2,input3]\n",
    "                                                                          \n",
    "                targets = np.asarray(self.output,dtype = np.float32)\n",
    "                                \n",
    "                self.reset()\n",
    "                \n",
    "                yield (inputs,targets)\n",
    "\n",
    "        \n",
    "    def next_val(self):\n",
    "        while True:\n",
    "            merge_input, distance = self.get_batch(self.X_val,self.y_val)\n",
    "            \n",
    "            self.batch1.append(merge_input[0])\n",
    "            self.batch2.append(merge_input[1])                                                                        \n",
    "            self.batch3.append(merge_input[2])\n",
    "                                                                      \n",
    "            self.output.append(distance)\n",
    "            \n",
    "            if len(self.output) == self.batch_size:\n",
    "                input1 = np.asarray(self.batch1, dtype=np.float32).reshape(self.batch_size,\n",
    "                                                                                   merge_input.shape[1],\n",
    "                                                                                    merge_input.shape[2],1)\n",
    "                \n",
    "                input2 = np.asarray(self.batch1, dtype=np.float32).reshape(self.batch_size,\n",
    "                                                                                   merge_input.shape[1],\n",
    "                                                                                    merge_input.shape[2],1)\n",
    "                \n",
    "                input3 = np.asarray(self.batch1, dtype=np.float32).reshape(self.batch_size,\n",
    "                                                                                   merge_input.shape[1],\n",
    "                                                                                    merge_input.shape[2],1)\n",
    "                \n",
    "                inputs = [input1,input2,input3]\n",
    "                                                                          \n",
    "                targets = np.asarray(self.output,dtype = np.float32)\n",
    "                                \n",
    "                self.reset()\n",
    "                \n",
    "                yield (inputs,targets)\n",
    "    \n",
    "    def test(self):\n",
    "        ## テストも同様かな。\n",
    "        pass\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_begin(self):\n",
    "        # metric learningではカリキュラム学習やdata augmentaioは必要ないかも\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "with open(\"03_work/label2int.pickle\",mode = \"rb\") as f:\n",
    "    label2int = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データの情報を読み込み\n",
    "train_info = pd.read_csv(\"01_input/ntt_corevo/class_train.tsv\",\n",
    "                  delimiter = \"\\t\",\n",
    "                  names = (\"filename\",\"label\"))\n",
    "\n",
    "# filepath変数を作成\n",
    "train_info[\"filepath\"] = \"01_input/ntt_corevo/train/\" + train_info[\"filename\"] + \".wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataの読み込み\n",
    "test_info = pd.read_csv(\"03_work/test_time_distribution.csv\")\n",
    "test_info[\"filepath\"] = \"01_input/ntt_corevo/\" + test_info[\"filename\"] + \".wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 0\n",
    "ub = 500\n",
    "batch_size = 2\n",
    "time_len = 500\n",
    "milestones = [1,2,3,4,5]\n",
    "\n",
    "data_gen = SpectrogramGeneratorMetricLearning(lb,ub,train_info,label2int,batch_size,time_len,test_info,milestones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i,v in enumerate(data_gen.next_train()):\\n#    print(v[1])\\n# metricではなく、distanceで距離を測定する必要がある?\\n    print(model.predict([v[0][0],v[0][1],v[0][2]]))\\n    print(metric.predict([v[0][0],v[0][1]]))\\n    if i == 100:\\n        break\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "for i,v in enumerate(data_gen.next_train()):\n",
    "#    print(v[1])\n",
    "# metricではなく、distanceで距離を測定する必要がある?\n",
    "    print(model.predict([v[0][0],v[0][1],v[0][2]]))\n",
    "    print(metric.predict([v[0][0],v[0][1]]))\n",
    "    if i == 100:\n",
    "        break\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワーク構造の可視化\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "def plot_keras_model(model, show_shapes=True,        show_layer_names=True):\n",
    "    return SVG(model_to_dot(model, show_shapes=show_shapes,         show_layer_names=show_layer_names).create(prog='dot',format='svg'))\n",
    "\n",
    "#plot_keras_model(model, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 2\n",
    "PATIENCE = 3\n",
    "\n",
    "save_dir = \"03_work/models/metric_learning/\"\n",
    "es = EarlyStopping(monitor = \"val_loss\",\n",
    "                   patience = PATIENCE)\n",
    "\n",
    "mc = ModelCheckpoint(save_dir + \"metric_learning.h5\",\n",
    "                    monitor = \"val_loss\",\n",
    "                    save_best_only = True,\n",
    "                    verbose = 1)\n",
    "\n",
    "# epochsの決め方が難しい。nC2*nC1　* 6??\n",
    "# とりあえず、１００００\n",
    "\n",
    "STEPS_PER_EPOCH = 10000\n",
    "VAL_STEPS_PER_EPOCH = 0.1 * STEPS_PER_EPOCH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "50/50 [==============================] - 63s 1s/step - loss: 0.7912 - val_loss: 0.7910\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.79105, saving model to 03_work/models/metric_learning/metric_learning.h5\n",
      "Epoch 2/3\n",
      "50/50 [==============================] - 41s 810ms/step - loss: 0.7909 - val_loss: 0.7907\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.79105 to 0.79065, saving model to 03_work/models/metric_learning/metric_learning.h5\n",
      "Epoch 3/3\n",
      "50/50 [==============================] - 40s 807ms/step - loss: 0.7905 - val_loss: 0.7903\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.79065 to 0.79027, saving model to 03_work/models/metric_learning/metric_learning.h5\n"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "hist = model.fit_generator(data_gen.next_train(),\n",
    "                          steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                           epochs= NUM_EPOCHS,\n",
    "                           validation_data = data_gen.next_val(),\n",
    "                           validation_steps =VAL_STEPS_PER_EPOCH,\n",
    "                           callbacks = [mc,es],\n",
    "                           verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習ログの保存\n",
    "for key in hist.history.keys():\n",
    "    np.savetxt(save_dir + \"/logs/{0}.txt\".format(key),\n",
    "               hist.history[key],\n",
    "               delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
